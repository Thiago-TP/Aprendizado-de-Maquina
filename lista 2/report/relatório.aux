\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Question 1}{2}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Item a}{2}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Item b}{2}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Item c}{2}{subsection.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Item d}{2}{subsection.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Item e}{2}{subsection.1.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  Illustration of PCA algorithm on a 2D dataset. Top row illustrates PCA projection, bottom row illustrates the reconstruction from those projections. The red vectors are tied to the samples' average, while blue vectors indicate the covariance matrix' orthonormal eigenvectors. Yellow dots are the projections of samples onto the most expressive eigenvector (arrows for some samples omitted for readability). }}{3}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:pca-illustration}{{2}{3}{Illustration of PCA algorithm on a 2D dataset. Top row illustrates PCA projection, bottom row illustrates the reconstruction from those projections. The red vectors are tied to the samples' average, while blue vectors indicate the covariance matrix' orthonormal eigenvectors. Yellow dots are the projections of samples onto the most expressive eigenvector (arrows for some samples omitted for readability)}{figure.caption.2}{}}
\newlabel{fig:pca-illustration@cref}{{[figure][2][]2}{[1][2][]3}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  Original data (black) versus PCA 1D (blue) and 2D (red) reconstructions. Bottom row shows the same view angles as top row but at higher elevation. }}{4}{figure.caption.3}\protected@file@percent }
\newlabel{fig:pca-reconstructions}{{3}{4}{Original data (black) versus PCA 1D (blue) and 2D (red) reconstructions. Bottom row shows the same view angles as top row but at higher elevation}{figure.caption.3}{}}
\newlabel{fig:pca-reconstructions@cref}{{[figure][3][]3}{[1][4][]4}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Question 2}{5}{section.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  Iteration of the k-means algorithm on a simple 2D dataset. Feature space. Labels are set at random from the given samples and $k=2$, then updated according to the clusters' averages (centroids). Numbers in the bubbles indicate the relevant step in the numbered list. }}{5}{figure.caption.4}\protected@file@percent }
\newlabel{fig:kmeans-illustration}{{4}{5}{Iteration of the k-means algorithm on a simple 2D dataset. Feature space. Labels are set at random from the given samples and $k=2$, then updated according to the clusters' averages (centroids). Numbers in the bubbles indicate the relevant step in the numbered list}{figure.caption.4}{}}
\newlabel{fig:kmeans-illustration@cref}{{[figure][4][]4}{[1][5][]5}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  Implemented $k$-means algorithm. $\mathbf  {X}^*$ denotes the zero-mean, normalized dataset, and $\mathbf  {x}^*$ is a row from it. Values $r_j$ are random integers. Omitted: the returns of the algorithm, namely the clusters $\mathcal  {C}_j$, labels $\mathbf  {l}_j$, and reconstruction errors. }}{6}{figure.caption.5}\protected@file@percent }
\newlabel{fig:kmeans-flowchart}{{5}{6}{Implemented $k$-means algorithm. $\mathbf {X}^*$ denotes the zero-mean, normalized dataset, and $\mathbf {x}^*$ is a row from it. Values $r_j$ are random integers. Omitted: the returns of the algorithm, namely the clusters $\mathcal {C}_j$, labels $\mathbf {l}_j$, and reconstruction errors}{figure.caption.5}{}}
\newlabel{fig:kmeans-flowchart@cref}{{[figure][5][]5}{[1][6][]6}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  Case study: \texttt  {cat-10.jpg}, $k=15$. Original image, reconstructed image using k-means, reconstruction error, and clusterings in sample space. }}{7}{figure.caption.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces  Case study: \texttt  {cat-101.jpg}, $k=15$. Original image, reconstructed image using k-means, reconstruction error, and clusterings in sample space. }}{8}{figure.caption.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces  Case study: \texttt  {cat-110.jpg}, $k=15$. Original image, reconstructed image using k-means, reconstruction error, and clusterings in sample space. }}{9}{figure.caption.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces  Case study: \texttt  {flower-6.jpg}, $k=10$. Original image, reconstructed image using k-means, reconstruction error, and clusterings in sample space. }}{10}{figure.caption.9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces  Case study: \texttt  {flower-14.jpg}, $k=10$. Original image, reconstructed image using k-means, reconstruction error, and clusterings in sample space. }}{11}{figure.caption.10}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces  Case study: \texttt  {flower-23.jpg}, $k=10$. Original image, reconstructed image using k-means, reconstruction error, and clusterings in sample space. }}{12}{figure.caption.11}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces  Case study: \texttt  {horse-137.jpg}, $k=10$. Original image, reconstructed image using k-means, reconstruction error, and clusterings in sample space. }}{13}{figure.caption.12}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces  Case study: \texttt  {horse-139.jpg}, $k=15$. Original image, reconstructed image using k-means, reconstruction error, and clusterings in sample space. }}{14}{figure.caption.13}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces  Case study: \texttt  {horse-170.jpg}, $k=15$. Original image, reconstructed image using k-means, reconstruction error, and clusterings in sample space. }}{15}{figure.caption.14}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Question 3}{16}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Item a}{16}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Item b}{16}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Item c}{16}{subsection.3.3}\protected@file@percent }
\newlabel{eq:b_ki}{{10}{16}{Item c}{equation.10}{}}
\newlabel{eq:b_ki@cref}{{[equation][10][]10}{[1][16][]16}{}{}{}}
\newlabel{eq:y_ki}{{11}{17}{Item c}{equation.11}{}}
\newlabel{eq:y_ki@cref}{{[equation][11][]11}{[1][16][]17}{}{}{}}
\newlabel{eq:ys_ki}{{13}{17}{Item c}{equation.13}{}}
\newlabel{eq:ys_ki@cref}{{[equation][13][]13}{[1][17][]17}{}{}{}}
\newlabel{eq:argmin}{{16}{17}{Item c}{equation.16}{}}
\newlabel{eq:argmin@cref}{{[equation][16][]16}{[1][17][]17}{}{}{}}
\newlabel{eq:gradient}{{17}{17}{Item c}{equation.17}{}}
\newlabel{eq:gradient@cref}{{[equation][17][]17}{[1][17][]17}{}{}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces  Implementation of the logistic regression (training). Samples are assumed to be zero-mean and normalized. }}{18}{algocf.1}\protected@file@percent }
\newlabel{alg:logistic-regression}{{1}{18}{Item c}{algocf.1}{}}
\newlabel{alg:logistic-regression@cref}{{[algorithm][1][]1}{[1][18][]18}{}{}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces  Class prediction calculation algorithm, with $\mathbf  {u}_k$ the $k$-th row of $\mathbf  {U}$.}}{18}{algocf.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces  Correlation between attributes as a colormap. The original correlation matrix was reorganized as to cluster bigger correlations at the upper left corner. }}{19}{figure.caption.17}\protected@file@percent }
\newlabel{fig:correlations}{{15}{19}{Correlation between attributes as a colormap. The original correlation matrix was reorganized as to cluster bigger correlations at the upper left corner}{figure.caption.17}{}}
\newlabel{fig:correlations@cref}{{[figure][15][]15}{[1][18][]19}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces  Weight matrix as a colormap. Bottom row classifies ``male'', top ``female''. }}{19}{figure.caption.18}\protected@file@percent }
\newlabel{fig:weight-matrix}{{16}{19}{Weight matrix as a colormap. Bottom row classifies ``male'', top ``female''}{figure.caption.18}{}}
\newlabel{fig:weight-matrix@cref}{{[figure][16][]16}{[1][19][]19}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces  Histograms of attributes. Highly correlated (removed) attributes were colored either orange, green or red. }}{20}{figure.caption.19}\protected@file@percent }
\newlabel{fig:histograms}{{17}{20}{Histograms of attributes. Highly correlated (removed) attributes were colored either orange, green or red}{figure.caption.19}{}}
\newlabel{fig:histograms@cref}{{[figure][17][]17}{[1][19][]20}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces  F1-Score and ROC for (a) training and (b) testing. All rates refer to ``male'' class. }}{21}{figure.caption.20}\protected@file@percent }
\newlabel{fig:results-training}{{18a}{21}{\relax }{figure.caption.20}{}}
\newlabel{fig:results-training@cref}{{[subfigure][1][18]18a}{[1][21][]21}{}{}{}}
\newlabel{sub@fig:results-training}{{a}{21}{\relax }{figure.caption.20}{}}
\newlabel{sub@fig:results-training@cref}{{[subfigure][1][18]18a}{[1][21][]21}{}{}{}}
\newlabel{fig:results-testing}{{18b}{21}{\relax }{figure.caption.20}{}}
\newlabel{fig:results-testing@cref}{{[subfigure][2][18]18b}{[1][21][]21}{}{}{}}
\newlabel{sub@fig:results-testing}{{b}{21}{\relax }{figure.caption.20}{}}
\newlabel{sub@fig:results-testing@cref}{{[subfigure][2][18]18b}{[1][21][]21}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces  Confusion matrix of the regressor using $D=0.35$ in test set. }}{21}{figure.caption.21}\protected@file@percent }
\newlabel{fig:confusion-matrix}{{19}{21}{Confusion matrix of the regressor using $D=0.35$ in test set}{figure.caption.21}{}}
\newlabel{fig:confusion-matrix@cref}{{[figure][19][]19}{[1][21][]21}{}{}{}}
\gdef \@abspage@last{22}
